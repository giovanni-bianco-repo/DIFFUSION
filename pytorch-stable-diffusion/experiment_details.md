### Experiment 1 : 
**parameters**: lora_r = 4, lora_alpha = 1.0, lora_dropout = 0.0, batch_size = 4, epoch = 5, lr=1e-4, schedule: Linear, Timestep: Uniform <br>
**loss**: Epoch 1/5 - Avg loss: 0.1378  Epoch 2/5 - Avg loss: 0.1086  Epoch 3/5 - Avg loss: 0.1016  Epoch 4/5 - Avg loss: 0.0861  Epoch 5/5 - Avg loss: 0.0644 <br>
**Apply layers**: Both cross attention & self attention
